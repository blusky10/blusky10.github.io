{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/21/hello-world/"},{"title":"PV &amp; PVC","text":"Go to section PV PV클러스터 리소스 이다. pv 와 pvc 는 1:1 바인딩이며 pvc 가 요청하는 볼륨이 pv 에 없으면 무한 대기 한다. pod 이 사용중인 pvc 는 삭제가 불가능 하다. PV volumeModes Filesystem : Pod 의 디렉토리에 마운트 됨 Block PersistentVolumeReclaimPolicyPVC 삭제시 PV 데이터에 대한 정책 Retail : 그대로 보존 Recycle : 재사용시 기존 pv 데이터들 삭제 후 재사용 (이건 사용 안함) Delete : 볼륨 삭제 RecaimPolicy Updatekubectl patch pv -p ‘{“spec”:{“persistentVolumeReclaimPolicy”:”Retain”}}’ PV &amp; PVC YAML 샘플1234567891011121314151617181920212223242526apiVersion: v1kind: PersistentVolumemetadata: namespace: spring name: mysql-pvspec: storageClassName: local-path accessModes: - ReadWriteOnce capacity: storage: 2Gi hostPath: path: /home/master01/k8s/mysql-data---apiVersion: v1kind: PersistentVolumeClaimmetadata: namespace: spring name: mysql-pvc2spec: storageClassName: local-path1 accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 참고 사이트https://kubernetes.io/ko/docs/concepts/storage/persistent-volumes/https://kubernetes.io/ko/docs/tasks/administer-cluster/change-pv-reclaim-policy/","link":"/2020/10/20/PV-PVC/"},{"title":"ReplicaSet","text":"ReplicaSet vs Replication ControllerReplicaSet 은 Replicatation Controller 의 새로운 버전이다. ReplcaSet : Set-based Selectors Replicatation Controller : Equality-based Selectors support Operation Example Command Line manifest Equality-based Service, Replication Controller = == != enviroment=prd kubectl get pods -l enviroment=prd selector: enviroment: prd Set-based Job, Deployment, ReplicaSet, Daemon Set in notin exists enviroment in (prd) kubectl get pods -l ‘enviroment in (prd)’ selector: matchExpressions:- {key: enviroment, operation: In, values: [prd]} matchLabelsselectors 에 matchLabels 가 존재할 경우 새로운 리소스들까지 지원을 해준다. manifest support selector: app: nginx Services, Replication Controller selector: matchLabels: app: nginx ReplicaSets, Deployments, Jobs, DaemonSet Persistent Volume(PV) vs Psersistent Volume Claim(PVC)PV : Piece of Storage in Clusterlifecycle : Provisioning -&gt; Binding -&gt; Using -&gt; Reclaiming Type : static, dynamic static : PV needs to be created before PVCdynamic : PV is created at same time of PVC PVC : Request for storage","link":"/2020/10/20/ReplicaSet/"},{"title":"Game-Of-Pods-Bravo","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods 전부터 해봐야지 했던 Game Of Pod 를 이제야 끝마쳤다. 처음에는 templete 들을 잘 몰라서 시간이 많이 걸렸는데 차츰차츰 해결을 해나가다 보니 익숙해졌다. pv 와 pvc, service 들은 상당히 간단하다. Deployment 의 env와 initContainer 와 Secret 을 눈여겨볼 필요가 있다. Solutionsdrpal-pv-hostpath, drupal-mysql-pv-hostpath /drupal-mysql-data (create the directory on Worker Nodes)/drupal-data (create the directory on Worker Nodes) connect node01 : ssh node01 mkdir /drupal-mysql-data mkdir /drupal-data drupal-mysql-pv Volume Name: drupal-mysql-pvStorage: 5GiAccess modes: ReadWriteOnce yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata:name: drupal-mysql-pvspec:accessModes: - ReadWriteOncecapacity: storage: 5GihostPath: path: /drupal-mysql-data drupal-mysql-pvc Claim Name: drupal-mysql-pvcStorage Request: 5GiAccess modes: ReadWriteOnce yaml12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: drupal-mysql-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi drupal-mysql-secret Secret Name: drupal-mysql-secretSecret: MYSQL_ROOT_PASSWORD=root_passwordSecret: MYSQL_DATABASE=drupal-databaseSecret: MYSQL_USER=root 값 인코딩 123echo -n &quot;root_password&quot; | base64echo -n &quot;drupal-database&quot; | base64echo -n &quot;root&quot; | base64 yaml 123456789apiVersion: v1kind: Secretmetadata: name: drupal-mysql-secrettype: Opaquedata: MYSQL_ROOT_PASSWORD: cm9vdF9wYXNzd29yZA== MYSQL_DATABASE: ZHJ1cGFsLWRhdGFiYXNl MYSQL_USER: cm9vdA== command line 으로 입력 하는 방법 1kubectl create secret generic drupal-mysql-secret --from-literal=MYSQL_ROOT_PASSWORD=root_password --from-literal=MYSQL_DATABASE=drupal-database --from-literal=MYSQL_USER=root drupal-mysql Name: drupal-mysqlReplicas: 1Image: mysql:5.7Deployment Volume uses PVC : drupal-mysql-pvcVolume Mount Path: /var/lib/mysql, subPath: dbdataDeployment: ‘drupal-mysql’ running yaml1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: apps/v1kind: Deploymentmetadata: name: drupal-mysqlspec: replicas: 1 selector: matchLabels: app: drupal-mysql template: metadata: labels: app: drupal-mysql spec: containers: - name: drupal-mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_ROOT_PASSWORD - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_DATABASE - name: MYSQL_USER valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_USER ports: - containerPort: 3306 name: mysql protocol: TCP volumeMounts: - mountPath: &quot;/var/lib/mysql&quot; name: mysql-volume subPath: dbdata drupal-mysql-service Name: drupal-mysql-serviceType: ClusterIPPort: 3306 yaml1234567891011apiVersion: v1kind: Servicemetadata: name: drupal-mysql-servicespec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: app: drupal-mysql drupal-pv Access modes: ReadWriteOnceVolume Name: drupal-pvStorage: 5Gi yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata:name: drupal-pvspec:accessModes: - ReadWriteOncecapacity: storage: 5GihostPath: path: /drupal-data drupal-pvc Claim Name: drupal-pvcStorage Request: 5GiAccess modes: ReadWriteOnce yaml12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: drupal-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi drupal Deployment Name: drupalReplicas: 1Image: drupal:8.6Deployment has an initContainer, name: ‘init-sites-volume’initContainer ‘init-sites-volume’, image: drupal:8.6initContainer ‘init-sites-volume’, persistentVolumeClaim: drupal-pvcinitContainer ‘init-sites-volume’, mountPath: /datainitContainer ‘init-sites-volume’, Command: [ “/bin/bash”, “-c” ],initContainer: Args: [ ‘cp -r /var/www/html/sites/ /data/; chown www-data:www-data /data/ -R’ ]Deployment ‘drupal’ uses correct pvc: drupal-pvcDeployment has a regular container, name: ‘drupal’, image: ‘drupal:8.6’container: ‘drupal’, Volume mountPath: /var/www/html/modules, subPath: modulescontainer: ‘drupal’, Volume mountPath: /var/www/html/profiles, subPath: profilescontainer: ‘drupal’, Volume mountPath: /var/www/html/sites, subPath: sitescontainer: ‘drupal’, Volume mountPath: /var/www/html/themes, subPath: themesDeployment: “drupal” runningDeployment: ‘drupal’ has label ‘app=drupal’ yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: apps/v1 kind: Deploymentmetadata: name: drupal labels: app: drupal spec: replicas: 1 selector: matchLabels: app: drupal template: metadata: labels: app: drupal spec: initContainers: - name: init-sites-volume image: drupal:8.6 command: [ &quot;/bin/bash&quot;, &quot;-c&quot; ] args: [ 'cp -r /var/www/html/sites/ /data/; chown www-data:www-data /data/ -R' ] volumeMounts: - mountPath: &quot;/data&quot; name: drupal-volume containers: - name: drupal image: drupal:8.6 ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/var/www/html/modules&quot; name: drupal-volume subPath: modules - mountPath: &quot;/var/www/html/profiles&quot; name: drupal-volume subPath: profiles - mountPath: &quot;/var/www/html/sites&quot; name: drupal-volume subPath: sites - mountPath: &quot;/var/www/html/themes&quot; name: drupal-volume subPath: themes volumes: - name: drupal-volume persistentVolumeClaim: claimName: drupal-pvc drupal-service frontend service name: drupal-servicedrupal-service configured as NodePortdrupal-service uses NodePort 30095 yaml123456789101112apiVersion: v1kind: Servicemetadata: name: drupal-servicespec: type: NodePort ports: - protocol: TCP port: 80 nodePort: 30095 selector: app: drupal","link":"/2020/10/23/Game-Of-Pods-Bravo/"},{"title":"Game-Of-Pods-IronGallery","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutions 주의할점iron-gallery-service 생성시 nodePort 를 꼭 추가해야한다. iron-gallery-service 의 조건에는 없지만 iron-gallery-ingress 생성시 접속 주소에 nodePort 가 명시되어있다. iron-gallery New Deployment, name: ‘iron-gallery’Image: ‘kodekloud/irongallery:2.0’volume, name = config, type: emptyDirvolume, name = images, type: emptyDirvolumeMount, name: ‘config’, mountPath: ‘/usr/share/nginx/html/data’volumeMount, name: ‘images’, mountPath: ‘/usr/share/nginx/html/uploads’Replicas: 1Pod Label: ‘run=iron-gallery’ yaml123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: iron-galleryspec: replicas: 1 selector: matchLabels: run: iron-gallery template: metadata: labels: run: iron-gallery spec: containers: - image: kodekloud/irongallery:2.0 name: iron-gallery volumeMounts: - name: config mountPath: /usr/share/nginx/html/data - name: images mountPath: /usr/share/nginx/html/uploads volumes: - name: config emptyDir: {} - name: images emptyDir: {} iron-gallery-limits Deployment: ‘iron-gallery’, container has CPU limit: ‘50m’Deployment: ‘iron-gallery’, container has Memory limit: ‘100Mi’ 1kubectl set resources deploy iron-gallery --limits=cpu=50m,memory=100Mi netpol-iron-gallery NetworkPolicy, name: ‘iron-gallery-firewall’Ingress Rule - from Pod labeled: ‘run=iron-gallery’Applied to Pod with label: ‘db=mariadb’Applied to allow access to port : ‘3306’ yaml12345678910111213141516apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: iron-gallery-firewallspec: podSelector: matchLabels: db: mariadb ingress: - from: - podSelector: matchLabels: run: iron-gallery ports: - protocol: TCP port: 3306 iron-gallery-service Service: ‘iron-gallery-service’ has ‘one’ endpoint for pods in deployment ‘iron-gallery’?targetPort: 80port: 80 yaml123456789101112apiVersion: v1kind: Servicemetadata: name: iron-gallery-servicespec: ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30099 selector: run: iron-gallery gallery-of-braavos Ingress resource configured correctly and application accessible at ‘http://iron-gallery-braavos.com:30099/'Ingress Resource, name: ‘iron-gallery-ingress’host: iron-gallery-braavos.comhttp parth: ‘/‘http backend serviceName: ‘iron-gallery-service’Name: ingress-spacehttp backend servicePort: ‘80’ yaml12345678910111213apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: iron-gallery-ingressspec: rules: - host: iron-gallery-braavos.com http: paths: - path: / backend: serviceName: iron-gallery-service servicePort: 80 iron-db New Deployment, name: ‘iron-db’Image: ‘kodekloud/irondb:2.0’volume, name = db, type: emptyDirvolumeMount, name: ‘db’, mountPath: ‘/var/lib/mysql’Replicas: 1env, name: ‘MYSQL_ROOT_PASSWORD’, value: ‘Braavo’env, name: ‘MYSQL_DATABASE’, value: ‘lychee’env, name: ‘MYSQL_USER’, value: ‘lychee’env, name: ‘MYSQL_PASSWORD’, value: ‘lychee’Pod Label: ‘db=mariadb’ yaml1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: iron-dbspec: replicas: 1 selector: matchLabels: db: mariadb template: metadata: labels: db: mariadb spec: containers: - image: kodekloud/irondb:2.0 name: mariadb volumeMounts: - name: db mountPath: /var/lib/mysql env: - name: MYSQL_ROOT_PASSWORD value: Braavo - name: MYSQL_DATABASE value: lychee - name: MYSQL_USER value: lychee - name: MYSQL_PASSWORD value: lychee volumes: - name: db emptyDir: {} iron-db-service Service: ‘iron-db-service’ has ‘one’ endpoint for pods in deployment ‘iron-db’?targetPort: 3306Service Port: ‘3306’ yaml1234567891011apiVersion: v1kind: Servicemetadata: name: iron-db-servicespec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: db: mariadb","link":"/2020/10/23/Game-Of-Pods-IronGallery/"},{"title":"Game-Of-Pods-Pento","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutions cordon 지정된 노드에 더이상 Pod 들이 스케쥴링 되지 않도록 해준다. cordon 을 실행하면 node 의 STATUS 에 SchedulingDisabled 가 표시된다. uncordon 지정된 노드에 Pod 들이 스케쥴링 될수 있도록 해준다. master-node Master node: coredns deployment has image: ‘k8s.gcr.io/coredns:1.3.1’Fix kube-apiserver. Make sure its running and healthy.kubeconfig = /root/.kube/config, User = ‘kubernetes-admin’ Cluster: Server Port = ‘6443’ ca-file 경로 변경 /etc/kubernetes/manifests/kube-apiserver.yaml kube-apiserver.yaml 파일의 –client-ca-file 을 변경해준다. 변경전 : –client-ca-file=/etc/kubernetes/pki/ca-authority.crt 변경후 : –client-ca-file=/etc/kubernetes/pki/ca.crt ~/.kube/config 변경 cluster port 를 6443 으로 변경한다. 변경전 : server: https://172.17.0.28:2379 변경후 : server: https://172.17.0.28:6443 coredns Deployment 의 이미지를 변경한다. 1kubectl edit deployment coredns node01 node01 is ready and can schedule pods? uncordon 명령어 실행1kubectl uncordon node01 web node01 has hostPath created = ‘/web’ connect node01 : ssh node01 mkdir /web data-pv Create new PersistentVolume = ‘data-pv’PersistentVolume = data-pv, accessModes = ‘ReadWriteMany’PersistentVolume = data-pv, hostPath = ‘/web’PersistentVolume = data-pv, storage = ‘1Gi’ yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata: name: data-pvspec: accessModes: - ReadWriteMany hostPath: path: /web capacity: storage: 1Gi data-pvc Create new PersistentVolumeClaim = ‘data-pvc’PersistentVolume = ‘data-pvc’, accessModes = ‘ReadWriteMany’PersistentVolume = ‘data-pvc’, storage request = ‘1Gi’PersistentVolume = ‘data-pvc’, volumeName = ‘data-pv’ yaml1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: data-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi volumeName: data-pv gop-file-server Create a pod for fileserver, name: ‘gop-fileserver’pod: gop-fileserver image: ‘kodekloud/fileserver’pod: gop-fileserver mountPath: ‘/web’pod: gop-fileserver volumeMount name: ‘data-store’pod: gop-fileserver persistent volume name: data-storepod: gop-fileserver persistent volume claim used: ‘data-pvc’ yaml1234567891011121314151617apiVersion: v1kind: Podmetadata: labels: app: fs name: gop-fileserverspec: containers: - name: fs image: kodekloud/fileserver volumeMounts: - mountPath: /web name: data-store volumes: - name: data-store persistentVolumeClaim: claimName: data-pvc gop-fs-service New Service, name: ‘gop-fs-service’Service name: gop-fs-service, port: ‘8080’Service name: gop-fs-service, targetPort: ‘8080’ yaml123456789101112131415apiVersion: v1kind: Servicemetadata: labels: app: fs name: gop-fs-servicespec: ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 31200 selector: app: fs type: NodePort","link":"/2020/10/23/Game-Of-Pods-Pento/"},{"title":"Game-Of-Pods-VotingApp","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutionscreate namespace1kubectl create ns vote vote-deployment Create a deployment: name = ‘vote-deployment’image = ‘kodekloud/examplevotingapp_vote:before’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: vote-deployment namespace: vote spec: selector: matchLabels: app: vote-deployment template: metadata: labels: app: vote-deployment spec: containers: - name: vote-deployment image: kodekloud/examplevotingapp_vote:before vote-service Create a new service: name = vote-serviceport = ‘5000’targetPort = ‘80’nodePort= ‘31000’service endpoint exposes deployment ‘vote-deployment’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: vote-service namespace: votespec: ports: - protocol: TCP port: 5000 targetPort: 80 nodePort: 31000 type: NodePort selector: app: vote-deployment redis-deployment Create new deployment, name: ‘redis-deployment’image: ‘redis:alpine’Volume Type: ‘EmptyDir’Volume Name: ‘redis-data’mountPath: ‘/data’status: ‘Running’ yaml1234567891011121314151617181920212223apiVersion: apps/v1kind: Deploymentmetadata: name: redis-deployment namespace: votespec: selector: matchLabels: app: redis-deployment template: metadata: labels: app: redis-deployment spec: containers: - name: redis-deployment image: redis:alpine volumeMounts: - name: redis-data mountPath: /data volumes: - name: redis-data emptyDir: {} redis New Service, name = ‘redis’port: ‘6379’targetPort: ‘6379’type: ‘ClusterIP’service endpoint exposes deployment ‘redis-deployment’ yaml123456789101112apiVersion: v1kind: Servicemetadata: name: redis namespace: votespec: ports: - protocol: TCP port: 6379 targetPort: 6379 selector: app: redis-deployment worker Create new deployment. name: ‘worker’image: ‘kodekloud/examplevotingapp_worker’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: worker namespace: vote spec: selector: matchLabels: app: worker template: metadata: labels: app: worker spec: containers: - image: 'kodekloud/examplevotingapp_worker' name: worker db-deployment Create new deployment. name: ‘db-deployment’image: ‘postgres:9.4’Volume Type: ‘EmptyDir’Volume Name: ‘db-data’mountPath: ‘/var/lib/postgresql/data’status: ‘Running’ yaml123456789101112131415161718192021222324252627282930apiVersion: apps/v1kind: Deploymentmetadata: name: db-deployment namespace: vote spec: selector: matchLabels: app: db-deployment template: metadata: labels: app: db-deployment spec: containers: - image: postgres:9.4 name: db-deployment volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data ports: - containerPort: 5432 name: db protocol: TCP env: - name: POSTGRES_PASSWORD value: test volumes: - name: db-data emptyDir: {} db Create new service: ‘db’port: ‘5432’targetPort: ‘5432’type: ‘ClusterIP’ yaml123456789101112apiVersion: v1kind: Servicemetadata: name: db namespace: votespec: ports: - protocol: TCP port: 5432 targetPort: 5432 selector: app: db-deployment result-deployment Create new deployment, name: ‘result-deployment’image: ‘kodekloud/examplevotingapp_result:before’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: result-deployment namespace: vote spec: selector: matchLabels: app: result-deployment template: metadata: labels: app: result-deployment spec: containers: - image: kodekloud/examplevotingapp_result:before name: db-deployment result-service port: ‘5001’targetPort: ‘80’NodePort: ‘31001’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: result-service namespace: votespec: ports: - protocol: TCP port: 5001 targetPort: 80 nodePort: 31001 type: NodePort selector: app: result-deployment","link":"/2020/10/23/Game-Of-Pods-VotingApp/"},{"title":"Game-Of-Pods-Tyro","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutionscreate namespace1kubectl create ns development admin Certificate and key pair for user drogo is created under /root. Add this user to kubeconfig = /root/.kube/config, User = drogo, client-key = /root/drogo.key client-certificate = /root/drogo.crtCreate a new context in the default config file (/root/.kube/config) called ‘developer’ with user = drogo and cluster = kubernetes 12kubectl config set-credentials drogo --client-key=/root/drogo.key --client-certificate=/root/drogo.crtkubectl config set-context developer --cluster=kubernetes --user=drogo developer-role ‘developer-role’, should have all() permissions for services in development namespace‘developer-role’, should have all permissions() for persistentvolumeclaims in development namespace‘developer-role’, should have all(*) permissions for pods in development namespace yaml1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: developer-role namespace: developmentrules: - apiGroups: - &quot;&quot; resources: - svc - pvc - pods verbs: - &quot;*&quot; developer-rolebinding create rolebinding = developer-rolebinding, role= ‘developer-role’, namespace = developmentrolebinding = developer-rolebinding associated with user = ‘drogo’ yaml 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: developer-rolebinding namespace: developmentsubjects: - kind: User name: drogo apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: developer-role apiGroup: rbac.authorization.k8s.io kube-config set context ‘developer’ with user = ‘drogo’ and cluster = ‘kubernetes’ as the current context. 1kubectl config use-context developer jekyll-pvc Storage Request: 1GiAccess modes: ReadWriteManypvc name = jekyll-site, namespace development yaml 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jekyll-site namespace: developmentspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi jekyll pod: ‘jekyll’ has an initContainer, name: ‘copy-jekyll-site’, image: ‘kodekloud/jekyll’initContainer: ‘copy-jekyll-site’ command: [ “jekyll”, “new”, “/site” ] (command to run: jekyll new /site)pod: ‘jekyll’, initContainer: ‘copy-jekyll-site’, mountPath = /sitepod: ‘jekyll’, initContainer: ‘copy-jekyll-site’, volume name = sitepod: ‘jekyll’, container: ‘jekyll’, volume name = sitepod: ‘jekyll’, container: ‘jekyll’, mountPath = /sitepod: ‘jekyll’, container: ‘jekyll’, image = kodekloud/jekyll-servepod: ‘jekyll’, uses volume called ‘site’ with pvc = ‘jekyll-site’pod: ‘jekyll’ uses label ‘run=jekyll’ yaml12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: namespace: development name: jekyll labels: run: jekyllspec: initContainers: - name: copy-jekyll-site image: kodekloud/jekyll command: [ &quot;jekyll&quot;, &quot;new&quot;, &quot;/site&quot; ] volumeMounts: - mountPath: /site name: site containers: - name: jekyll image: kodekloud/jekyll-serve volumeMounts: - mountPath: /site name: site volumes: - name: site persistentVolumeClaim: claimName: jekyll-site jekyll-node-service Service ‘jekyll’ uses targetPort: ‘4000’ , namespace: ‘development’Service ‘jekyll’ uses Port: ‘8080’ , namespace: ‘development’Service ‘jekyll’ uses NodePort: ‘30097’ , namespace: ‘development’ yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: name: jekyll namespace: development labels: run: jekyllspec: ports: - protocol: TCP port: 8080 targetPort: 4000 nodePort: 30097 selector: run: jekyll type: NodePort","link":"/2020/10/23/Game-Of-Pods-Tyro/"},{"title":"Game-Of-Pods-RedisIslands","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods SolutionsStatefulset 에 대한 내용을 알고 있어야 한다. redis01~06 PersistentVolume - Name: redis01Access modes: ReadWriteOnceSize: 1GihostPath: /redis01, directory should be created on worker node ssh node01 make /redis01~06 yaml (숫자만 01에서 06까지 바꿔주면 된다)1234567891011apiVersion: v1kind: PersistentVolumemetadata: name: redis01spec: accessModes: - ReadWriteOnce hostPath: path: /redis01 capacity: storage: 1Gi redis-cluster StatefulSet - Name: redis-clusterReplicas: 6Pods status: Running (All 6 replicas)Image: redis:5.0.1-alpinecontainer name: redis, command: [“/conf/update-node.sh”, “redis-server”, “/conf/redis.conf”]Env: name: ‘POD_IP’, valueFrom: ‘fieldRef’, fieldPath: ‘status.podIP’ (apiVersion: v1)Ports - name: ‘client’, containerPort: ‘6379’Ports - name: ‘gossip’, containerPort: ‘16379’Volume Mount - name: ‘conf’, mountPath: ‘/conf’, readOnly:’false’ (ConfigMap Mount)Volume Mount - name: ‘conf’, mountPath: ‘/conf’, defaultMode = ‘0755’ (ConfigMap Mount)Volume Mount - name: ‘data’, mountPath: ‘/data’, readOnly:’false’ (volumeClaim)volumes - name: ‘conf’, Type: ‘ConfigMap’, ConfigMap Name: ‘redis-cluster-configmap’,volumeClaimTemplates - name: ‘data’volumeClaimTemplates - accessModes: ‘ReadWriteOnce’volumeClaimTemplates - Storage Request: ‘1Gi’ yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: apps/v1kind: StatefulSetmetadata: name: redis-clusterspec: selector: matchLabels: app: redis-cluster serviceName: redis-cluster replicas: 6 template: metadata: labels: app: redis-cluster spec: containers: - name: redis image: redis:5.0.1-alpine command: [&quot;/conf/update-node.sh&quot;, &quot;redis-server&quot;, &quot;/conf/redis.conf&quot;] env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP ports: - name: client containerPort: 6379 - name: gossip containerPort: 16379 volumeMounts: - name: conf mountPath: /conf readOnly: false - name: data mountPath: /data readOnly: false volumes: - name: conf configMap: name: redis-cluster-configmap defaultMode: 0755 volumeClaimTemplates: - metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi redis-cluster-config Configure the Cluster. Once the StatefulSet has been deployed with 6 ‘Running’ pods, run the below commands and type ‘yes’ when prompted.Command: kubectl exec -it redis-cluster-0 – redis-cli –cluster create –cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath=’{range.items[*]}{.status.podIP}:6379 ‘) 아래 명령어 실행1kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 ') redis-cluster-service Ports - service name ‘redis-cluster-service’, port name: ‘client’, port: ‘6379’Ports - service name ‘redis-cluster-service’, port name: ‘gossip’, port: ‘16379’Ports - service name ‘redis-cluster-service’, port name: ‘client’, targetPort: ‘6379’Ports - service name ‘redis-cluster-service’, port name: ‘gossip’, targetPort: ‘16379’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: redis-cluster-servicespec: ports: - protocol: TCP name: client port: 6379 targetPort: 6379 - protocol: TCP name: gossip port: 16379 targetPort: 16379","link":"/2020/10/23/Game-Of-Pods-RedisIslands/"},{"title":"쿠버네티스입문 5장 파드","text":"Pod개념쿠버네티스에서 실제로 컨테이너를 묶어서 관리하는 단위 설정123456789101112apiVersion: v1kind: Podmetadata: name: simple-pod (Pod 이름) labels: app: simple-pod (오브젝트를 식별하는 레이블)spec: containers: - name: simple-pod (컨테이너 이름) image: ~~~ (컨네이너에서 사용할 이미지) ports: - containerPort: 8080 Pod 생명주기Pending -&gt; Running Successed Failed Unknown 컨테이너 진단 ivenessProbe 컨테이너가 실행됐는지 확인 실패시 컨테이너를 종료시키고 재시작 정책에 따라서 재시작 readinessProbe 컨테이너 실행된 후 실제로 서비스 요청에 응답할 수 있는지 진단 컨테이너가 실제로 트래픽을 받을 준비가 되었음을 확인할 수 있어 유용함 초기화 컨테이너(Init Container) 앱 컨테이너가 실행 되기 전 Pod 를 초기화 한다. 여러개 구성 가능하며 실행 순서는 템플릿에 명시한 순서를 따른다. 실패시 성공할때 까지 재시작한다. readinessProbe 를 지원하지 않는다 - Pod 가 준비되기 전에 실행후 종료되기 때문 설정 .spec.initContainers[] 의 하위 필드 1234567891011apiVersion: v1kind: Podmetadata: name: simple-pod labels: app: simple-podspec: initContainers: - name: image: command 파드 인프라 컨테이너pause 컨테이너 Pod Infrastructure Container (파드 인프라 컨테이너) 모든 Pod 에서 항상 실행된다 다른 컨테이너의 부모 역할을 한다 Pod 안 다른 컨테이너들은 pause 컨테이너가 제공하는 네트워크를 사용 pause 컨테이너 재시작시 Pod 안 모든 컨테이너 재시작됨12c34dff11289b arisu1000/simple-container-app &quot;./simple-container-…&quot; 41 minutes ago Up 41 minutes k8s_simple-pod_simple-pod_default_112e5cb1-101b-4cb6-a591-c83c6171cce5_067c790777792 k8s.gcr.io/pause:3.2 &quot;/pause&quot; 41 minutes ago Up 41 minutes k8s_POD_simple-pod_default_112e5cb1-101b-4cb6-a591-c83c6171cce5_0 스태틱 Pod kube-apiserver 를 통하지 않고 kubelet 이 직접 실행하는 Pod 조회는 가능하지만 명령 실행은 불가능(직접 edit 등 불가능) 시스템 파드 실행용도로 많이 사용 스태틱 Pod 경로 : /etc/kubernetes/manifests 자원 할당 requests 최소자원, limits 최대자원 .spec.containers[].resources.limits.cpu .spec.containers[].resources.limits.memory .spec.containers[].resources.requests.cpu .spec.containers[].resources.requests.memory requests 만 설정했을 경우 해당 값보다 더 많이 사용할 수 있기 때문에 limits 설정을 해야 Out of Memory 를 피할수 있다. Memory : Exa, Peta, Tera, Giga, Mega, Kilo (맨 첫글자 사용) CPU : 소수점일 경우 1개 코어에 해당하는 연산능력을 의미( 0.1 일경우 1코어의 10%만 활용) Pod 의 환경 변수.spec.conainers[].env[] 하위 필드 name value valueFrom fieldRef fieldPath resourceFieldRef containerName resource 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/23/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-5%EC%9E%A5-%ED%8C%8C%EB%93%9C/"},{"title":"쿠버네티스입문 6장 컨트롤러","text":"Controller개념Pod 를 관리하는 역학을 한다. Replicatoin Controller(레플리케이션 컨트롤러), ReplicaSet(레플리카 셋)Replication Contller 초기부터 있었던 기본적인 컨트롤러 명시한 Pod 개수만큼 유지하도록 해준다. 현재는 ReplcaSet 을 쓴다. ReplicaSet 레플리케이션 컨트롤러의 발전형. 레플리케이션 컬트롤러와 차이점은 집합기반 셀렉터를 지원 한다. (in, notin, exists) rolling-update 옵션 사용불가 설정1234567891011121314151617181920apiVersion: v1kind: ReplicaSetmetadata: name: nginx-replicasetspec: template: metadata: name: nginx-replicaset labels: app: nginx-replicaset spec: containers: - name: nginx-replicaset image: nginx ports: - containePort: 80 replicas: 3 selector: matchLabels: app: nginx-replicase .spec.template.metadata.labels 의 설정과 spec.selector.matchLabels의 설정이 같아야 한다. selector 설정이 없을 경우 .spec.template.metadata.labels 를 따라간다. 레플리카셋 삭제시 –cascade=false 옵션을 하면 레플리카셋만 삭제 가능하다. (Pod 삭제안됨) Deployment(디플로이먼트) Stateless 앱 배포시 사용하는 기본적인 컨트롤러 레플리카셋을 관리한다. 설정 12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginx-deploymentspec: replicas: 3 selector: matchLabels: app: nginx-deployment template: metadata: name: nginx-deployment labels: app: nginx-deployment spec: containers: - name: nginx-deployment image: nginx ports: - containerPort: 80 생성시 Deployemnt, ReplicaSet, Pod 가 생성된다. 설정정보 Update 방법 kubectl set kubectl edit yaml 파일 수정후 apply revsion kubectl rollout history deployment 이름 kubectl rollout undo deploy (이전 revision 으로 롤백) kubectl rollout undo deploy 이름 –to-revision=숫자 (특정 revision 으로 롤백) Pod 개수 조정 kubectl scale deploy 이름 –replicas=숫자 배포 정지, 재개, 재시작 kubectl rollout pause deployment/이름 kubectl rollout resume deployment/이름 Daemonset (데몬셋) 클러스터 전체 노드에 특정 파드를 실행할 때 사용하는 컨트롤러 클러스터 전체에 항상 실행시켜두어야 하는 파드에 사용 설정123456789101112131415161718192021222324252627282930apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch updateStrategy: type: RollingUpdate template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: fluent/fluentd-kubernetes-daemonset:elasticsearch env: - name: testenv value: value resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi .spec.updateStrategy.type : 다음 두가지중 선택 OnDelete : 파드가 삭제되었을때 반영된다. RollingUpdate : 기본값 (1.5 이하에서는 OnDelete 가 기본값) 템플릿 변경시 바로 반영 모든 파드가 한꺼번에 반영되는건 아니고 .spec.updateStrategy.rollingUpdate.maxUnavailable 필드와 .spec.minReadySeconds 필드를 추가로 설정해 한번에 교체하는 파드를 조정한다. StatefulSets (스테이트풀 셋) 상태가 있는 파드들을 관리하는 컨트롤러 생성될때 Pod 에 UUID 가 붙는게 아니라 숫자(0,1,2..)가 붙는다. 삭제될때에는 숫자가 큰것부터 삭제가 된다. (업데이트시에는 Pod를 삭제하고 다시 생성하기 때문에 마찬가지로 숫자가 큰것부터 수정된다.) Job (잡) 실행된후 종료해야 하는 성격의 작업을 실행할때 사용하는 컨트롤러 설정12345678910111213apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: [] restartPolicy: Naver backoffLimit: 4 .spec.completions : 정상적으로 실행 종료 되어야 하는 파드 개수 .spec.parallelism : 동시에 실행 가능한 파드 개수 .spec.restartPolicy : 재시작 정책을 설정한다. 크론잡 job 을 시간 기준으로 관리한다. 주기적으로 반복이 가능하다. 설정123456789101112131415161718apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello form the k8s restartPolicy: OnFailure .spec.schedule : cron 명령 설정과 동일 (위는 1분) 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/23/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-6%EC%9E%A5-%EC%BB%A8%ED%8A%B8%EB%A1%A4%EB%9F%AC/"},{"title":"쿠버네티스입문-7장-서비스","text":"Service개념 동적으로 변하는 Pod 들을 고정적으로 접근할 때 사용한다. 서비스는 주로 L4영역에서 통신할 때 사용한다. 서비스 타입 ClusterIP default, 클러스터 내부에서만 사용가능 NodePort 모 모든 노드에 지정된 포트를 할당함. 외부에서 접근 가능 LoadBalancer EXTERNAL-IP 생성 외부에서 Pod 접근 가능 할수 있게 해줌 ExternalName 서비스를 .spec.externalName 필드에 설정한 값과 연결한다. 클러스터 안에서 외부에 접근할 때 주로 사용한다. 서비스 사용 설정123456789101112apiVersion: v1kind: Servicemetadata: name: my-servicespec: type: ClusterIP selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 8080 .spec.clusterIP 값을 None 로 설정하면 IP 가 없는 서비스 생성 가능 kube-proxy userspace 모드 Pod 연결 요청시 실패할 경우 다른 Pod에 연결을 재시도함. iptables 모드 클라이언트 요청을 iptables 를 거쳐 Pod 로 직접 전달 Pod 연결 요청시 실패할 경우 재시도 안함 IPVS 모드 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/28/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-7%EC%9E%A5-%EC%84%9C%EB%B9%84%EC%8A%A4/"},{"title":"쿠버네티스입문 8장 인그레스","text":"Ingress개념 클러스터 외부에서 안으로 접근하는 요청들을 어떻게 처리할지 정의해둔 규칙 인그레스는 규칙들의 모음이며 실제로는 인그레스 컨트롤러가 동작시킨다. 설정1234567891011121314151617181920212223242526272829303132333435apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: test annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - host: foo.bar.com http: paths: - path: /foos1 pathType: Prefix backend: service: name: s1 port: number: 80 - path: /bars2 pathType: Prefix backend: service: name: s2 port: number: 80 - host: bar.foo.com http: paths: - path: / pathType: Prefix backend: service: name: s2 port: number: 80 annotation 은 인그레스 컨트롤러마다 다르다. (https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/) ingress version 차이 1.18 123456789101112131415apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: serviceName: test servicePort: 80 1.19 1234567891011121314151617apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 SSL 설정 tls 생성1234567openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=kube-book.com&quot;Can't load /root/.rnd into RNG140377540018624:error:2406F079:random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/root/.rndGenerating a RSA private key........................+++++........+++++writing new private key to 'tls.key' tls secret 생성123 kubectl create secret tls kube-book-secret --key tls.key --cert tls.crt ~~~ - .spec.tls 설정 spec:tls: hosts: kube-boo.comsecretName: kube-book-secret 무중단 배포시 주의 할점 maxSurge 와 maxUnavailable 설정 maxSurge : 디플로이먼트에 기본 pod 개수에 여분의 파드를 몇개 추가 할수 있는지 설정 maxUnavailable : 디플로이먼트 업데이트시 사용할수 없는 파드 개수 readinessProbe 확인 실제 서비스 요청 처리할 준비가 되었는지 진단 설정 불가능할 경우 .spec.minReadySeconds 설정. 해당 설정 기간동은 트래픽을 받지 않지만 그 이후에는 받는다. graceful 종료 기존 받은 요청만 처리하고 새 요청은 받지 않는다 설정 불가능 할 경우 hock 을 설정한다. 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/28/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-8%EC%9E%A5-%EC%9D%B8%EA%B7%B8%EB%A0%88%EC%8A%A4/"},{"title":"쿠버네티스입문 10장 컨피그맵","text":"ConfigMap 컨테이너에 필요한 환경 설정을 컨테이너와 분리해서 제공하는 기능 사용 설정123456789apiVersion: v1kind: ConfigMapmetadata: name: config-devdata: DB_URL: localhost DB_USER: myuser DB_PASS: mypass DEBUG_INFO: debug 컨피그맵 일부만 사용12345678910spec: containers: - name: image: env: - name: DEBUG_LEVEL valueFrom: configMapKeyRef: name: config-dev key : DEBUG_INFO .env[].valueFrom 사용 .env[].valueFrom.configMapKeyRef 를 통해 이미 정의된 configmap 사용 컨피그맵 전체를 불러오기1234567spec: containers: - name: image: envFrom: - configMapRef: name: config-dev volume 에 바인딩 하기1234567891011spec: containers: - name: image: volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: config-dev 컨테이너 내부에 파일로 저장한다.12root@nginx-deployment-67b8444cdf-sp7lx:/# ls /etc/config/DB_PASS DB_URL DB_USER DEBUG_INFO 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/30/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-10%EC%9E%A5-%EC%BB%A8%ED%94%BC%EA%B7%B8%EB%A7%B5/"},{"title":"쿠버네티스입문 11장 시크릿","text":"Secret 비밀번호 같은 민감한 정보를 저장하는 용도로 사용 생성 명령어 kubectl create generic secret명 –from-file~ 실제 값은 base64 로 인코딩한 값이 들어간다. 템플릿12345678apiVersion: v1kind: Secretmetadata: name: user-pass-yamltype: Opaquedata: username: 값 password: 값 type Opaque : 기본값 kubernetes.io/service-account-token : 쿠버네티스 인증토큰 저장 kubernetes.io/dockerconfigjson: 도커 저장소 인증정보 저장 kubernetes.io/tls: TLS 인증서 저장 data 값은 base64 로 인코딩 한 값을 넣어야 한다. echo -n “username” | base64 사용 환경변수로 사용12345678910spec: containers: - name: image: env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: user-pass-yaml key: username volume 에 바인딩 하기123456789101112spec: containers: - name: image: volumeMounts: - name: volume-secret mountPath: /etc/config readOnly: true volumes: - name: config-secret secret: secretName: user-pass-yaml 프라이빗 커네이너 이미지 pull kubectl create secret docker-registry dockersecret –docker-username= –docker-password= –docker-email= –docker-service=https://~ 설정123456spec:containers:- name: image:imagePullSecrets:- name: dockersecret TLS 사용 kubectl create secret tls tlssecret –key tls.key –cert tls.crt 제한 secret 은 etcd 에 저장된다. secret의 최대 용량은 1MB 작은 용량의 secret 을 여러개 만들어도 문제가 생길수 있다. etcd 는 접근 제한을 해야 한다. 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/30/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-11%EC%9E%A5-%EC%8B%9C%ED%81%AC%EB%A6%BF/"},{"title":"쿠버네티스입문 9장 레이블과 애너테이션","text":"Label, AnnotationLabel(레이블) 키-값 쌍으로 구성 파드 관리할때 구분하는 역할을 한다. 규칙 63글자 넘으면 안됨 시작과 끝문자는 알파벳 대소문자 및 숫자 중간에는 대시(-), 밑줄(_), 점(.), 숫자등이 올수 있음 레이블 셀렉터 등호기반(=, ==) 집합기반(in, notin..) 레이블을 모두 만족시켜야 하는경우 (And) 는 쉼표로 연결 실제 서비스에서 정상적으로 셀렉트를 했는지 보려면 서비스의 endpoint를 확인해보면 된다. pod 선택시 -l 옵션 사용 kubectl get pod -l 레이블~ Annotation 쿠버네티스 시스템이 필요한 정보를 담는다. 키는 쿠버네티스 시스템이 인식할 수 있는 값을 사용한다. 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/30/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-9%EC%9E%A5-%EB%A0%88%EC%9D%B4%EB%B8%94%EA%B3%BC-%EC%95%A0%EB%84%88%ED%85%8C%EC%9D%B4%EC%85%98/"},{"title":"쿠버네티스입문 12장 파드스케줄링","text":"파드 스케줄링파드를 만들때 어떤 노드에서 실행할지 다양한 옵션으로 선택할 수 있다. NodeSelector(노드 셀렉터) 설정1234spec: containers: nodeSelector: key: value Node Affinity 노드 레이블을 기반으로 파드를 스케줄링 한다. 노드 셀렉터를 함께 설정할 경우 둘다 만족하는 노드에 스케줄링된다. requiredDuringSchedulingIgnoredDuringExecution : 스케줄링 하는동안 꼭 필요한 조건 preferredDuringSchedulingIgnoredDuringExecution : 만족하면 좋은 조건. (필수아님) 스케줄링 하는 동안 조건이 변경되더라도 무시한다. 설정123456789101112131415161718192021spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/os operator: In values: - linux - window - key: disktype operator: Exists preferredDuringSchedulingIgnoredDuringExecution - weight: 10 preference: matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker-node01 operators : In, NotIn, Exists, DoesNotExist, Gt(Greater than), Lt(Lower than) Pod affinity, antiAffinity affinity: 서로 연관성이 있는 Pod 를 같은 노드에 배치 antiAffinity: 자원을 많이 차지 하는 Pod 를 서로 다른 노드에 배치 설정1234567891011spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; app 필드 값이 store 인 노드는 피해서 스케줄링 한다. topologyKey : affinity 를 만족하면 topologyKey 기준으로 같은 노드에 pod 를 실행하고 anitiAffinity를 만족하면 topologyKey 기준으로 다른 노드에 pod 를 실행한다. taint, toleration 테인트를 설정한 노드는 스케줄링 하지 않는다. 테인트를 설정한 노드에 스케줄링 하려면 톨러레이션을 설정해야 한다. kubectl taint nodes 노드명 key01=value01:NoSchedule (key=value:효과) kubectl taint nodes 노드명 key01=value01:NoSchedule- (테인트 제거) 톨러레이션123456spec: tolerations: - key: &quot;key01&quot; operator: &quot;Equal&quot; value: &quot;value01&quot; effect: &quot;NoSchedule&quot; effect NoSchedule : 톨러레이션 설정 없으면 스케줄링 안함. PreferNoSchedule : 스케줄링 안하지만 자원 부족하면 할수도 있음 NoExecute : 스케줄링 안하며 기존 파드도 설정이 없는경우 종료시킴 cordon, drain cordon 지정한 노드에 추가로 파드를 스케줄링 하지 않는다. kubectl cordon 노드명master01@master01-VirtualBox:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master01-virtualbox Ready master 79d v1.19.2 worker01-virtualbox Ready &lt;none&gt; 79d v1.19.2 worker02-virtualbox NotReady &lt;none&gt; 79d v1.19.2 master01@master01-VirtualBox:~$ kubectl cordon worker01-virtualbox node/worker01-virtualbox cordoned master01@master01-VirtualBox:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master01-virtualbox Ready master 79d v1.19.2 worker01-virtualbox Ready,SchedulingDisabled &lt;none&gt; 79d v1.19.2 worker02-virtualbox NotReady &lt;none&gt; 79d v1.19.2 master01@master01-VirtualBox:~$ kubectl uncordon worker01-virtualbox node/worker01-virtualbox uncordoned master01@master01-VirtualBox:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master01-virtualbox Ready master 79d v1.19.2 worker01-virtualbox Ready &lt;none&gt; 79d v1.19.2 worker02-virtualbox NotReady &lt;none&gt; 79d v1.19.2 drain 지정된 노드에 있는 파드를 다른 노드로 옮긴다. 노드에 데몬셋이 존재할 경우 적용 불가능 데몬셋 무시하고 적용할 경우 –ignore-daemonsets=true 옵션 추가 컨트롤러를 이용하지 않은 파드들도 drain 불가능 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2020/12/31/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-12%EC%9E%A5-%ED%8C%8C%EB%93%9C%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/"},{"title":"쿠버네티스입문 13장 인증과 권한","text":"인증과 권한kubectl config ~/.kube/config 파일 12345678910111213141516171819apiVersion: v1clusters:- cluster: certificate-authority-data: ~~ server: https://10.0.1.7:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: {}users:- name: kubernetes-admin user: client-certificate-data: data client-key-data: key cluster.server: 외부에서 접속할수 있는 정보 clusters[].name: 클러스터 이름 context.cluster: 접근할 클러스터 context.user: 클러스터에 접근할 사용자 그룹 contexts[].name: 컨텍스트 이름 users[]: 클러스터를 사용할 사용자 그룹 user.client-certificate-data: 클라이언트 인증에 필요한 해시값 user.client-key-data: 클라이언트 키 해시값 token 조회 하기 123456789101112master01@master01-VirtualBox:~$ kubectl get sa default -o yamlapiVersion: v1kind: ServiceAccountmetadata: creationTimestamp: &quot;2020-10-12T10:21:28Z&quot; name: default namespace: default resourceVersion: &quot;395&quot; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 034e5b6a-0484-4aef-a78f-42a206032779secrets:- name: default-token-w5t42 default-token~이라는 secret확인 가능1234567891011121314kubectl describe secret default-token-w5t42Name: default-token-w5t42Namespace: defaultLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 034e5b6a-0484-4aef-a78f-42a206032779Type: kubernetes.io/service-account-tokenData====namespace: 7 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6ImNIV1I5a0MtZVUxUE1JY0RHY1lvRkJZZDN5M3o3TkxncWY3QUN3R0FGelkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tdzV0NDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjAzNGU1YjZhLTA0ODQtNGFlZi1hNzhmLTQyYTIwNjAzMjc3OSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.irtlJ4PmyJZ-5auePspxP4xd0yMF8_aAVQN3gWRBaWeCgkYlkOWgS13EfwyKQEoQai2uGtL751G0eo07-VBvR0nV6ith0knHw3Ji2nYBzt0BSVke7DpLu4T6m6uSeNIPhugZ3QAJxB73tNS2BoCXM4Hpwvz8gyrmBMzT2ce7905XP8mXJwYV1-MnwlZFngSrnG6cHAqA6u1B4DpGSjZ1viLe_HL0j4tHn4NCRTEkxnJcEiSz4pcr2Frb1K2ZXVyhWslPQeIdA5QWaJEVvUe0GQK1V1aHRhfSYHfmJlSM00iJmrwCiY-fFMxvDiT4PCjLAwj4Bdoqjt-WsL_FeNWBXQca.crt: 1066 bytes 실제 토큰을 조회해 보면 위와 같다 토큰은 사용자를 추가 할때 사용될 수 있다. 권한 ABAC 속성기반 권한 관리 변경하려면 직접 마스터에 접속해 파일 변경후 apiserver 재시작해야한다. RBAC 역할 기반 권한 관리 사용자와 역할을 분리해서 선언후 바인딩한다. Role 특정 API 나 자원 사용 권한들을 명시해둔 규칙의 집합 Role과 ClusterRole 이 있다 Role 해당 롤이 속한 네임스페이스에만 적용된다. 설정1234567891011121314151617 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: read-role rules: - apiGroups: [&quot;&quot;] resources: [&quot;pod&quot;] verbs: [&quot;get&quot;, &quot;list&quot;] ~~~ - apiGroups: 롤이 사용할 apiGroup 들 - resouces: 접근할 수 있는 자원 - verbs: 어떤 동작이 가능한지 (create, get, list, update, patch, delete, deletecollection) - rules[].resourcesNames: 특정 이름의 리소스에만 적용 가능하게 할 수 있다. (get, delete, update, patch 만 적용가능하다. 나머지는 개별 api 를 호출하는 verbs 가 아니기 때문이다)- ClusterRole - 설정은 Role 과 동일 하나 namespace 항목은 빠진다. - .aggregationRule 설정을 통해 다른 클러스터롤과 조합 가능 aggregationRule: clusterRuleSelectors: matchLabels: kubernetes.io/bootstrapping: rbac-defaults123456 - rulese 를 설정 하지 않아도 다른 클러스터 롤에서 불러와서 적용한다. - 자원 뿐만 아니라 url 형태의 규칙도 추가 가능하며 verbs 는 get, post 만 사용 가능하다.- RoleBinding - 롤과 사용자를 연결한다. RoleBinding, ClusterRoleBinding 이 있다. - RoleBinding - 설정 apiVersion: v1kind: ServiceAccountmetadata:name: myusernamespace: default apiVersion: rbac.authorization/k8s.io/v1kind: RoleBindingmetadata: name: read-rolebinding namespace: defaultsubjects: kind: ServiceAccountname: myuserapiGroup: “”roleRef:kind: Rolename: read-roleapiGroup: rbac.authorization.k8s.io12345 - subjects[].apiGroup: &quot;&quot; 은 핵심 apiGroup으로 설정했다는 의미 - roleRef 는 어떤 롤을 할당할 건지 설정 - 롤바인딩에 설정한 네임스페이스에 한해서 가능- ClusterRoleBinding - 설정 apiVersion: rbac.authorization/k8s.io/v1kind: ClusterRoleBindingmetadata:name: read-clusterrolebindingsubjects: kind: ServiceAccountname: myusernamespace: defaultapiGroup: “”roleRef:kind: ClusterRolename: read-clusterroleapiGroup: rbac.authorization.k8s.io~ 롤바인딩과 차이점은 .subjects[].kind: ServiceAccount, User, Group 을 설정가능. ServiceAccount, User 설정시에는 네임스페이스 필요함 출처 : 쿠버네티스 입문 - 90가지 예제로 배우는 컨테이너 관리자 자동화 표준 (동양북스)","link":"/2021/01/04/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-13%EC%9E%A5-%EC%9D%B8%EC%A6%9D%EA%B3%BC-%EA%B6%8C%ED%95%9C/"},{"title":"쿠버네티스입문 14장 볼륨","text":"볼륨데이터를 보존 해야 하는 경우 사용한다. 볼륨 .spec.container.volumeMounts.mountPropagation 볼륨을 공유할지 여부를 설정한다. None : 호스트에서 볼륨에 해당하는 디렉터리 하위에 마운트한 다른 마운트들은 볼수 없다. HostToContainer : 호스트에서 해당 볼륨 하위에 마운트된 다른 디렉터리들도 볼수 있다. Bidirectional : 하위 마운트된 디렉토리를 볼수 있고 다른 모든 컨테이너나 파드에서 같은 볼륨을 사용할 수 있다. emptyDir 호스트의 디스크를 임시로 컨테이너에 볼륨으로 할당 파드가 사라지면 emptyDir 에 할당해서 사용했던 볼륨의 데이터도 사라진다 설정12345678910spec: containers: - name: kubernetes-simple-pod image: arisu1000/simple-container-app:latest volumMounts: - mountPath: /emptydir name: emptydir-vol volumes: - name: emptydir-vol emptyDir: {} 대부분 볼륨 설정과 마운트하는 부분이 분리되어있음. hostPath 파드가 실행된 호스트의 파일이나 디렉토리 마운트 emptyDir VS hostPath emptyDir hostPath 임시디렉토리 마운트 실제디렉토리 마운트 컨테이너 재시작했을때 데이터 보존 파드 재시작했을때 데이터 보존 설정 123456789101112spec: containers: - name: kubernetes-simple-pod image: arisu1000/simple-container-app:latest volumMounts: - mountPath: /test-volume name: hostpath-vol volumes: - name: hostpath-vol hostPath: path: /tmp type: Directory type DirectoryOrCreate : 디렉토리 없으면 퍼미션 755 로 디렉토리 생성 Directory : 해당 위치에 디렉토리 존재해야한다. FileOrCreate : 파일 없으면 퍼미션 644로 파일 생성 File : 설정 경로에 파일이 있어야함. Socket : 설정한 경로에 유닉스 소켓파일이 있어야함. CharDevice : 설정한 경로에 문자 디파이스 있는지 확인. BlockDevice : 설정한 경로에 블록 디바이스가 있는지 확인 nfs 기존에 사용하는 nfs 서버를 이용해서 파드에 마운트하는 것. 여러개의 파드에 볼륨 하나를 공유해 읽기/쓰기 동시에 할때도 사용한다. PersistentVolume &amp; PersistentVolumeClaim PV 는 볼륨 자체를 의미한다. 별도의 생명주기를 가지고 있다. PVC 는 사용자가 PV 에 하는 요청이다. PV 와 PVC 의 생명주기 프로비저닝(Provisioning) PV 를 만드는 단계 정적 : 미리 적정 용량의 PV 를 만들어둠 동적 : 사용자가 PVC 를 거쳐 PV를 요청했을때 생성 바인딩(Binding) PV 와 PVC 를 연결하는 단계 PV 와 PVC 는 1:1 관계이다 사용(Using) 할당된 PVC 는 시스템에서 임의로 삭제할 수 없다. 반환(Reclaiming) PVC 가 삭제되고 사용중인 PV를 초기화한다. 반환정책 Retain PV 를 그대로 보존한다. PV를 삭제 하더라도 외부 연결된 스토리지는 그대로 남는다. 재사용을 위해서는 PV를 삭제하고 스토리지에 남은 데이터를 직접 처리해줘야 한다. Delete PV 를 삭제하고 외부 스토리지의 볼륨도 삭제한다. Recycle PV의 데이터를 삭제하고 새로운 PVC 에서 PV 를 사용할 수 있도록 한다. 중단 예정 정책 PV 템플릿 설정 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: pv-hostpathspec: capacity: storage: 2Gi volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: manual persistentVolumeReclaimPolicy: Delete hostPath: path: /tmp/k8s-pv .spec.volumeMode: 볼륨의 설정 상태. 기본은 Filesystem .spec.accessModes: 읽기/쓰기 옵션 ReadWriteOnce: 노드 하나에만 읽기/쓰기로 마운트 ReadOnlyMany: 여러개 노드에 읽기 전용으로 마운트 ReadWriteMany: 여러개 노드에 읽기/쓰기로 마운트 플러그인 별로 다름 .spec.stroageClassName: 스토리지 클래스 설정. 특정 스토리지 클래스가 있는 PV 는 해당 클래스에 맞는 PVC 와 연결 가능하다. PVC 템플릿 설정123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-hostpathspec: volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: manual resources: requests: storage: 1Gi .spec.resources.requests.storage: 필드 자원을 얼마나 사용할 것인지 요청. pv의 용량을 초과하면 안된다. pv, pvc 연결 label 로 연결12345678910111213pv---metadata: name: pv-hostpath-label labels: location: local---pvc---spec: selector: matchLabels: location: local 파드에서 사용12345678910spec: containers: - name: simple-app volumeMounts: - mountPath: &quot;/tmp&quot; name: myvolume volumes: - name: myvolume persistentVolumeClaim: claimName: pvc-hostpath pvc 크기 변경 .spec.storageClassName.allowVolumeExpansion : true 로 설정 특정 볼륨 플러그인에 한해서만 가능 파일시스템을 사용하는 파드라면 새로운 파드를 실행할때에만 진행된다","link":"/2021/01/11/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-14%EC%9E%A5-%EB%B3%BC%EB%A5%A8/"},{"title":"쿠버네티스입문 15장 클러스터 네트워킹 구성","text":"클러스터 네트워킹 구성파드 네트워킹 파드에 속한 컨테이너들은 veth0 하나를 공유하기 때문에 모두 같은 IP 를 갖고 있다. 파드의 veth0 는 pause 컨테이너 네임스페이스에 속한 장치이다. 파드안 컨테이너들은 로컬주소로 서로 접속가능하다. 파드 안에서 구분은 포트로 한다. 멀티노드일 경우 각각의 호스트 내의 파드 IP 를 다른 노드의 파드 IP와 동일하게 하는건 좋은 방법이 아니다. CNI 를 통해서 호스트 네트워크 네임스페이스의 각종 네트워크 기능들을 사용한다. 서비스 네트워킹 NodePort 사용시 서비스의 엔드포인트가 있는 호스트에 NAT 테이블이 생성됨 사용자가 지정한 서비스용 IP 를 Pod의 IP와 연결해주는데 kube-proxy가 이 기능을 담당한다.","link":"/2021/01/14/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-15%EC%9E%A5-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%82%B9-%EA%B5%AC%EC%84%B1/"},{"title":"쿠버네티스입문 16장 쿠버네티스 DNS","text":"쿠버네티스 DNS클러스터 안에서 도메인 사용하기 서비스 도메인 패턴 : 서비스이름.네임스페이스이름.svc.cluster.local 파드 도메인 패턴 : 파드ip주소.네임스페이스이름.pod.cluster.local 파드 ip 주소는 -로 연결한다 (ex : 10.10.10.10 -&gt; 10-10-10-10) 10-10-10-10.default.pod.cluster.local .spec.template.spec.hostname 과 .spec.template.spec.subdomain 으로 설정한다. 파드의 도메인은 hostname.subdomain.네임스페이스이름.svc.cluster.local pod.cluster.local 이 아닌 svc.cluster.local 이다 dns 확인12345678910111213➜ ~ k get poNAME READY STATUS RESTARTS AGEmyapp-74db4764fd-8jnnz 1/1 Running 0 66s➜ ~ k exec myapp-74db4764fd-8jnnz nslookup appname.default-subdomain.default.svc.cluster.localkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.Name: appname.default-subdomain.default.svc.cluster.localAddress 1: 10.1.0.100 appname.default-subdomain.default.svc.cluster.localnslookup: can't resolve '(null)': Name does not resolve➜ ~ k get po -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-74db4764fd-8jnnz 1/1 Running 0 3m45s 10.1.0.100 docker-desktop &lt;none&gt; &lt;none&gt;➜ ~ DNS 질의 구조 파드마다 DNS를 어떤 순서로 질의할지 설정가능 .spec.dnsPolicy 필드 사용 Default ClusterFisrt ClusterFisrtWithHostNet None kube-dns kubedns, dnsmasq, sidecar 로 구성 kubedns 는 마스터를 보다가 엔드포인트 변경사항 발생시 메모리에 저장중인 DNS 데이터를 변경한다. sidecar는 dnsmasq와 kubedns 헬스체크를 실행한다. CoreDNS123456789101112131415161718192021222324252627282930➜ ~ k describe configmap coredns -n kube-systemName: corednsNamespace: kube-systemLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====Corefile:----.:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance}","link":"/2021/01/20/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-16%EC%9E%A5-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-DNS/"},{"title":"쿠버네티스입문 17장로깅과 모니터링","text":"로깅과 모니터링","link":"/2021/01/26/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%9E%85%EB%AC%B8-17%EC%9E%A5%EB%A1%9C%EA%B9%85%EA%B3%BC-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81/"}],"tags":[{"name":"K8S","slug":"K8S","link":"/tags/K8S/"},{"name":"PV","slug":"PV","link":"/tags/PV/"},{"name":"PVC","slug":"PVC","link":"/tags/PVC/"},{"name":"쿠버네티스","slug":"쿠버네티스","link":"/tags/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"},{"name":"ReplicaSet","slug":"ReplicaSet","link":"/tags/ReplicaSet/"},{"name":"RS","slug":"RS","link":"/tags/RS/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"kodekolud","slug":"kodekolud","link":"/tags/kodekolud/"},{"name":"gameofpod","slug":"gameofpod","link":"/tags/gameofpod/"},{"name":"Bravo","slug":"Bravo","link":"/tags/Bravo/"},{"name":"IronGallery","slug":"IronGallery","link":"/tags/IronGallery/"},{"name":"Pento","slug":"Pento","link":"/tags/Pento/"},{"name":"cordon","slug":"cordon","link":"/tags/cordon/"},{"name":"uncordon","slug":"uncordon","link":"/tags/uncordon/"},{"name":"VotingApp","slug":"VotingApp","link":"/tags/VotingApp/"},{"name":"Tyro","slug":"Tyro","link":"/tags/Tyro/"},{"name":"RedisIslands","slug":"RedisIslands","link":"/tags/RedisIslands/"},{"name":"쿠버네티스 입문","slug":"쿠버네티스-입문","link":"/tags/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EC%9E%85%EB%AC%B8/"},{"name":"pod","slug":"pod","link":"/tags/pod/"},{"name":"Book","slug":"Book","link":"/tags/Book/"},{"name":"동양북스","slug":"동양북스","link":"/tags/%EB%8F%99%EC%96%91%EB%B6%81%EC%8A%A4/"},{"name":"90가지 예제로 배우는 컨테이너 관리 자동화 표준","slug":"90가지-예제로-배우는-컨테이너-관리-자동화-표준","link":"/tags/90%EA%B0%80%EC%A7%80-%EC%98%88%EC%A0%9C%EB%A1%9C-%EB%B0%B0%EC%9A%B0%EB%8A%94-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EA%B4%80%EB%A6%AC-%EC%9E%90%EB%8F%99%ED%99%94-%ED%91%9C%EC%A4%80/"},{"name":"컨트롤러","slug":"컨트롤러","link":"/tags/%EC%BB%A8%ED%8A%B8%EB%A1%A4%EB%9F%AC/"},{"name":"Controller","slug":"Controller","link":"/tags/Controller/"},{"name":"서비스","slug":"서비스","link":"/tags/%EC%84%9C%EB%B9%84%EC%8A%A4/"},{"name":"Service","slug":"Service","link":"/tags/Service/"},{"name":"인그레스","slug":"인그레스","link":"/tags/%EC%9D%B8%EA%B7%B8%EB%A0%88%EC%8A%A4/"},{"name":"Ingress","slug":"Ingress","link":"/tags/Ingress/"},{"name":"컨피그맵","slug":"컨피그맵","link":"/tags/%EC%BB%A8%ED%94%BC%EA%B7%B8%EB%A7%B5/"},{"name":"configMap","slug":"configMap","link":"/tags/configMap/"},{"name":"시크릿","slug":"시크릿","link":"/tags/%EC%8B%9C%ED%81%AC%EB%A6%BF/"},{"name":"secret","slug":"secret","link":"/tags/secret/"},{"name":"레이블","slug":"레이블","link":"/tags/%EB%A0%88%EC%9D%B4%EB%B8%94/"},{"name":"애너테이션","slug":"애너테이션","link":"/tags/%EC%95%A0%EB%84%88%ED%85%8C%EC%9D%B4%EC%85%98/"},{"name":"파드스케줄링","slug":"파드스케줄링","link":"/tags/%ED%8C%8C%EB%93%9C%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/"},{"name":"볼륨","slug":"볼륨","link":"/tags/%EB%B3%BC%EB%A5%A8/"},{"name":"volume","slug":"volume","link":"/tags/volume/"},{"name":"클러스터 네트워킹","slug":"클러스터-네트워킹","link":"/tags/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%82%B9/"},{"name":"DNS","slug":"DNS","link":"/tags/DNS/"},{"name":"로깅","slug":"로깅","link":"/tags/%EB%A1%9C%EA%B9%85/"},{"name":"모니터링","slug":"모니터링","link":"/tags/%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81/"}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"Book","slug":"Kubernetes/Book","link":"/categories/Kubernetes/Book/"}]}