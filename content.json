{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/21/hello-world/"},{"title":"PV &amp; PVC","text":"Go to section PV PV클러스터 리소스 이다. pv 와 pvc 는 1:1 바인딩이며 pvc 가 요청하는 볼륨이 pv 에 없으면 무한 대기 한다. pod 이 사용중인 pvc 는 삭제가 불가능 하다. PV volumeModes Filesystem : Pod 의 디렉토리에 마운트 됨 Block PersistentVolumeReclaimPolicyPVC 삭제시 PV 데이터에 대한 정책 Retail : 그대로 보존 Recycle : 재사용시 기존 pv 데이터들 삭제 후 재사용 (이건 사용 안함) Delete : 볼륨 삭제 RecaimPolicy Updatekubectl patch pv -p ‘{“spec”:{“persistentVolumeReclaimPolicy”:”Retain”}}’ PV &amp; PVC YAML 샘플1234567891011121314151617181920212223242526apiVersion: v1kind: PersistentVolumemetadata: namespace: spring name: mysql-pvspec: storageClassName: local-path accessModes: - ReadWriteOnce capacity: storage: 2Gi hostPath: path: /home/master01/k8s/mysql-data---apiVersion: v1kind: PersistentVolumeClaimmetadata: namespace: spring name: mysql-pvc2spec: storageClassName: local-path1 accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 참고 사이트https://kubernetes.io/ko/docs/concepts/storage/persistent-volumes/https://kubernetes.io/ko/docs/tasks/administer-cluster/change-pv-reclaim-policy/","link":"/2020/10/20/PV-PVC/"},{"title":"ReplicaSet","text":"ReplicaSet vs Replication ControllerReplicaSet 은 Replicatation Controller 의 새로운 버전이다. ReplcaSet : Set-based Selectors Replicatation Controller : Equality-based Selectors support Operation Example Command Line manifest Equality-based Service, Replication Controller = == != enviroment=prd kubectl get pods -l enviroment=prd selector: enviroment: prd Set-based Job, Deployment, ReplicaSet, Daemon Set in notin exists enviroment in (prd) kubectl get pods -l ‘enviroment in (prd)’ selector: matchExpressions:- {key: enviroment, operation: In, values: [prd]} matchLabelsselectors 에 matchLabels 가 존재할 경우 새로운 리소스들까지 지원을 해준다. manifest support selector: app: nginx Services, Replication Controller selector: matchLabels: app: nginx ReplicaSets, Deployments, Jobs, DaemonSet Persistent Volume(PV) vs Psersistent Volume Claim(PVC)PV : Piece of Storage in Clusterlifecycle : Provisioning -&gt; Binding -&gt; Using -&gt; Reclaiming Type : static, dynamic static : PV needs to be created before PVCdynamic : PV is created at same time of PVC PVC : Request for storage","link":"/2020/10/20/ReplicaSet/"},{"title":"Game-Of-Pods-Bravo","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods 전부터 해봐야지 했던 Game Of Pod 를 이제야 끝마쳤다. 처음에는 templete 들을 잘 몰라서 시간이 많이 걸렸는데 차츰차츰 해결을 해나가다 보니 익숙해졌다. pv 와 pvc, service 들은 상당히 간단하다. Deployment 의 env와 initContainer 와 Secret 을 눈여겨볼 필요가 있다. Solutionsdrpal-pv-hostpath, drupal-mysql-pv-hostpath /drupal-mysql-data (create the directory on Worker Nodes)/drupal-data (create the directory on Worker Nodes) connect node01 : ssh node01 mkdir /drupal-mysql-data mkdir /drupal-data drupal-mysql-pv Volume Name: drupal-mysql-pvStorage: 5GiAccess modes: ReadWriteOnce yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata:name: drupal-mysql-pvspec:accessModes: - ReadWriteOncecapacity: storage: 5GihostPath: path: /drupal-mysql-data drupal-mysql-pvc Claim Name: drupal-mysql-pvcStorage Request: 5GiAccess modes: ReadWriteOnce yaml12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: drupal-mysql-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi drupal-mysql-secret Secret Name: drupal-mysql-secretSecret: MYSQL_ROOT_PASSWORD=root_passwordSecret: MYSQL_DATABASE=drupal-databaseSecret: MYSQL_USER=root 값 인코딩 123echo -n &quot;root_password&quot; | base64echo -n &quot;drupal-database&quot; | base64echo -n &quot;root&quot; | base64 yaml 123456789apiVersion: v1kind: Secretmetadata: name: drupal-mysql-secrettype: Opaquedata: MYSQL_ROOT_PASSWORD: cm9vdF9wYXNzd29yZA== MYSQL_DATABASE: ZHJ1cGFsLWRhdGFiYXNl MYSQL_USER: cm9vdA== command line 으로 입력 하는 방법 1kubectl create secret generic drupal-mysql-secret --from-literal=MYSQL_ROOT_PASSWORD=root_password --from-literal=MYSQL_DATABASE=drupal-database --from-literal=MYSQL_USER=root drupal-mysql Name: drupal-mysqlReplicas: 1Image: mysql:5.7Deployment Volume uses PVC : drupal-mysql-pvcVolume Mount Path: /var/lib/mysql, subPath: dbdataDeployment: ‘drupal-mysql’ running yaml1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: apps/v1kind: Deploymentmetadata: name: drupal-mysqlspec: replicas: 1 selector: matchLabels: app: drupal-mysql template: metadata: labels: app: drupal-mysql spec: containers: - name: drupal-mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_ROOT_PASSWORD - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_DATABASE - name: MYSQL_USER valueFrom: secretKeyRef: name: drupal-mysql-secret key: MYSQL_USER ports: - containerPort: 3306 name: mysql protocol: TCP volumeMounts: - mountPath: &quot;/var/lib/mysql&quot; name: mysql-volume subPath: dbdata drupal-mysql-service Name: drupal-mysql-serviceType: ClusterIPPort: 3306 yaml1234567891011apiVersion: v1kind: Servicemetadata: name: drupal-mysql-servicespec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: app: drupal-mysql drupal-pv Access modes: ReadWriteOnceVolume Name: drupal-pvStorage: 5Gi yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata:name: drupal-pvspec:accessModes: - ReadWriteOncecapacity: storage: 5GihostPath: path: /drupal-data drupal-pvc Claim Name: drupal-pvcStorage Request: 5GiAccess modes: ReadWriteOnce yaml12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: drupal-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi drupal Deployment Name: drupalReplicas: 1Image: drupal:8.6Deployment has an initContainer, name: ‘init-sites-volume’initContainer ‘init-sites-volume’, image: drupal:8.6initContainer ‘init-sites-volume’, persistentVolumeClaim: drupal-pvcinitContainer ‘init-sites-volume’, mountPath: /datainitContainer ‘init-sites-volume’, Command: [ “/bin/bash”, “-c” ],initContainer: Args: [ ‘cp -r /var/www/html/sites/ /data/; chown www-data:www-data /data/ -R’ ]Deployment ‘drupal’ uses correct pvc: drupal-pvcDeployment has a regular container, name: ‘drupal’, image: ‘drupal:8.6’container: ‘drupal’, Volume mountPath: /var/www/html/modules, subPath: modulescontainer: ‘drupal’, Volume mountPath: /var/www/html/profiles, subPath: profilescontainer: ‘drupal’, Volume mountPath: /var/www/html/sites, subPath: sitescontainer: ‘drupal’, Volume mountPath: /var/www/html/themes, subPath: themesDeployment: “drupal” runningDeployment: ‘drupal’ has label ‘app=drupal’ yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: apps/v1 kind: Deploymentmetadata: name: drupal labels: app: drupal spec: replicas: 1 selector: matchLabels: app: drupal template: metadata: labels: app: drupal spec: initContainers: - name: init-sites-volume image: drupal:8.6 command: [ &quot;/bin/bash&quot;, &quot;-c&quot; ] args: [ 'cp -r /var/www/html/sites/ /data/; chown www-data:www-data /data/ -R' ] volumeMounts: - mountPath: &quot;/data&quot; name: drupal-volume containers: - name: drupal image: drupal:8.6 ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/var/www/html/modules&quot; name: drupal-volume subPath: modules - mountPath: &quot;/var/www/html/profiles&quot; name: drupal-volume subPath: profiles - mountPath: &quot;/var/www/html/sites&quot; name: drupal-volume subPath: sites - mountPath: &quot;/var/www/html/themes&quot; name: drupal-volume subPath: themes volumes: - name: drupal-volume persistentVolumeClaim: claimName: drupal-pvc drupal-service frontend service name: drupal-servicedrupal-service configured as NodePortdrupal-service uses NodePort 30095 yaml123456789101112apiVersion: v1kind: Servicemetadata: name: drupal-servicespec: type: NodePort ports: - protocol: TCP port: 80 nodePort: 30095 selector: app: drupal","link":"/2020/10/23/Game-Of-Pods-Bravo/"},{"title":"Game-Of-Pods-IronGallery","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutions 주의할점iron-gallery-service 생성시 nodePort 를 꼭 추가해야한다. iron-gallery-service 의 조건에는 없지만 iron-gallery-ingress 생성시 접속 주소에 nodePort 가 명시되어있다. iron-gallery New Deployment, name: ‘iron-gallery’Image: ‘kodekloud/irongallery:2.0’volume, name = config, type: emptyDirvolume, name = images, type: emptyDirvolumeMount, name: ‘config’, mountPath: ‘/usr/share/nginx/html/data’volumeMount, name: ‘images’, mountPath: ‘/usr/share/nginx/html/uploads’Replicas: 1Pod Label: ‘run=iron-gallery’ yaml123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: iron-galleryspec: replicas: 1 selector: matchLabels: run: iron-gallery template: metadata: labels: run: iron-gallery spec: containers: - image: kodekloud/irongallery:2.0 name: iron-gallery volumeMounts: - name: config mountPath: /usr/share/nginx/html/data - name: images mountPath: /usr/share/nginx/html/uploads volumes: - name: config emptyDir: {} - name: images emptyDir: {} iron-gallery-limits Deployment: ‘iron-gallery’, container has CPU limit: ‘50m’Deployment: ‘iron-gallery’, container has Memory limit: ‘100Mi’ 1kubectl set resources deploy iron-gallery --limits=cpu=50m,memory=100Mi netpol-iron-gallery NetworkPolicy, name: ‘iron-gallery-firewall’Ingress Rule - from Pod labeled: ‘run=iron-gallery’Applied to Pod with label: ‘db=mariadb’Applied to allow access to port : ‘3306’ yaml12345678910111213141516apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: iron-gallery-firewallspec: podSelector: matchLabels: db: mariadb ingress: - from: - podSelector: matchLabels: run: iron-gallery ports: - protocol: TCP port: 3306 iron-gallery-service Service: ‘iron-gallery-service’ has ‘one’ endpoint for pods in deployment ‘iron-gallery’?targetPort: 80port: 80 yaml123456789101112apiVersion: v1kind: Servicemetadata: name: iron-gallery-servicespec: ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30099 selector: run: iron-gallery gallery-of-braavos Ingress resource configured correctly and application accessible at ‘http://iron-gallery-braavos.com:30099/'Ingress Resource, name: ‘iron-gallery-ingress’host: iron-gallery-braavos.comhttp parth: ‘/‘http backend serviceName: ‘iron-gallery-service’Name: ingress-spacehttp backend servicePort: ‘80’ yaml12345678910111213apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: iron-gallery-ingressspec: rules: - host: iron-gallery-braavos.com http: paths: - path: / backend: serviceName: iron-gallery-service servicePort: 80 iron-db New Deployment, name: ‘iron-db’Image: ‘kodekloud/irondb:2.0’volume, name = db, type: emptyDirvolumeMount, name: ‘db’, mountPath: ‘/var/lib/mysql’Replicas: 1env, name: ‘MYSQL_ROOT_PASSWORD’, value: ‘Braavo’env, name: ‘MYSQL_DATABASE’, value: ‘lychee’env, name: ‘MYSQL_USER’, value: ‘lychee’env, name: ‘MYSQL_PASSWORD’, value: ‘lychee’Pod Label: ‘db=mariadb’ yaml1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: iron-dbspec: replicas: 1 selector: matchLabels: db: mariadb template: metadata: labels: db: mariadb spec: containers: - image: kodekloud/irondb:2.0 name: mariadb volumeMounts: - name: db mountPath: /var/lib/mysql env: - name: MYSQL_ROOT_PASSWORD value: Braavo - name: MYSQL_DATABASE value: lychee - name: MYSQL_USER value: lychee - name: MYSQL_PASSWORD value: lychee volumes: - name: db emptyDir: {} iron-db-service Service: ‘iron-db-service’ has ‘one’ endpoint for pods in deployment ‘iron-db’?targetPort: 3306Service Port: ‘3306’ yaml1234567891011apiVersion: v1kind: Servicemetadata: name: iron-db-servicespec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: db: mariadb","link":"/2020/10/23/Game-Of-Pods-IronGallery/"},{"title":"Game-Of-Pods-Pento","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutions cordon 지정된 노드에 더이상 Pod 들이 스케쥴링 되지 않도록 해준다. cordon 을 실행하면 node 의 STATUS 에 SchedulingDisabled 가 표시된다. uncordon 지정된 노드에 Pod 들이 스케쥴링 될수 있도록 해준다. master-node Master node: coredns deployment has image: ‘k8s.gcr.io/coredns:1.3.1’Fix kube-apiserver. Make sure its running and healthy.kubeconfig = /root/.kube/config, User = ‘kubernetes-admin’ Cluster: Server Port = ‘6443’ ca-file 경로 변경 /etc/kubernetes/manifests/kube-apiserver.yaml kube-apiserver.yaml 파일의 –client-ca-file 을 변경해준다. 변경전 : –client-ca-file=/etc/kubernetes/pki/ca-authority.crt 변경후 : –client-ca-file=/etc/kubernetes/pki/ca.crt ~/.kube/config 변경 cluster port 를 6443 으로 변경한다. 변경전 : server: https://172.17.0.28:2379 변경후 : server: https://172.17.0.28:6443 coredns Deployment 의 이미지를 변경한다. 1kubectl edit deployment coredns node01 node01 is ready and can schedule pods? uncordon 명령어 실행1kubectl uncordon node01 web node01 has hostPath created = ‘/web’ connect node01 : ssh node01 mkdir /web data-pv Create new PersistentVolume = ‘data-pv’PersistentVolume = data-pv, accessModes = ‘ReadWriteMany’PersistentVolume = data-pv, hostPath = ‘/web’PersistentVolume = data-pv, storage = ‘1Gi’ yaml1234567891011apiVersion: v1kind: PersistentVolumemetadata: name: data-pvspec: accessModes: - ReadWriteMany hostPath: path: /web capacity: storage: 1Gi data-pvc Create new PersistentVolumeClaim = ‘data-pvc’PersistentVolume = ‘data-pvc’, accessModes = ‘ReadWriteMany’PersistentVolume = ‘data-pvc’, storage request = ‘1Gi’PersistentVolume = ‘data-pvc’, volumeName = ‘data-pv’ yaml1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: data-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi volumeName: data-pv gop-file-server Create a pod for fileserver, name: ‘gop-fileserver’pod: gop-fileserver image: ‘kodekloud/fileserver’pod: gop-fileserver mountPath: ‘/web’pod: gop-fileserver volumeMount name: ‘data-store’pod: gop-fileserver persistent volume name: data-storepod: gop-fileserver persistent volume claim used: ‘data-pvc’ yaml1234567891011121314151617apiVersion: v1kind: Podmetadata: labels: app: fs name: gop-fileserverspec: containers: - name: fs image: kodekloud/fileserver volumeMounts: - mountPath: /web name: data-store volumes: - name: data-store persistentVolumeClaim: claimName: data-pvc gop-fs-service New Service, name: ‘gop-fs-service’Service name: gop-fs-service, port: ‘8080’Service name: gop-fs-service, targetPort: ‘8080’ yaml123456789101112131415apiVersion: v1kind: Servicemetadata: labels: app: fs name: gop-fs-servicespec: ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 31200 selector: app: fs type: NodePort","link":"/2020/10/23/Game-Of-Pods-Pento/"},{"title":"Game-Of-Pods-VotingApp","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutionscreate namespace1kubectl create ns vote vote-deployment Create a deployment: name = ‘vote-deployment’image = ‘kodekloud/examplevotingapp_vote:before’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: vote-deployment namespace: vote spec: selector: matchLabels: app: vote-deployment template: metadata: labels: app: vote-deployment spec: containers: - name: vote-deployment image: kodekloud/examplevotingapp_vote:before vote-service Create a new service: name = vote-serviceport = ‘5000’targetPort = ‘80’nodePort= ‘31000’service endpoint exposes deployment ‘vote-deployment’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: vote-service namespace: votespec: ports: - protocol: TCP port: 5000 targetPort: 80 nodePort: 31000 type: NodePort selector: app: vote-deployment redis-deployment Create new deployment, name: ‘redis-deployment’image: ‘redis:alpine’Volume Type: ‘EmptyDir’Volume Name: ‘redis-data’mountPath: ‘/data’status: ‘Running’ yaml1234567891011121314151617181920212223apiVersion: apps/v1kind: Deploymentmetadata: name: redis-deployment namespace: votespec: selector: matchLabels: app: redis-deployment template: metadata: labels: app: redis-deployment spec: containers: - name: redis-deployment image: redis:alpine volumeMounts: - name: redis-data mountPath: /data volumes: - name: redis-data emptyDir: {} redis New Service, name = ‘redis’port: ‘6379’targetPort: ‘6379’type: ‘ClusterIP’service endpoint exposes deployment ‘redis-deployment’ yaml123456789101112apiVersion: v1kind: Servicemetadata: name: redis namespace: votespec: ports: - protocol: TCP port: 6379 targetPort: 6379 selector: app: redis-deployment worker Create new deployment. name: ‘worker’image: ‘kodekloud/examplevotingapp_worker’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: worker namespace: vote spec: selector: matchLabels: app: worker template: metadata: labels: app: worker spec: containers: - image: 'kodekloud/examplevotingapp_worker' name: worker db-deployment Create new deployment. name: ‘db-deployment’image: ‘postgres:9.4’Volume Type: ‘EmptyDir’Volume Name: ‘db-data’mountPath: ‘/var/lib/postgresql/data’status: ‘Running’ yaml123456789101112131415161718192021222324252627282930apiVersion: apps/v1kind: Deploymentmetadata: name: db-deployment namespace: vote spec: selector: matchLabels: app: db-deployment template: metadata: labels: app: db-deployment spec: containers: - image: postgres:9.4 name: db-deployment volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data ports: - containerPort: 5432 name: db protocol: TCP env: - name: POSTGRES_PASSWORD value: test volumes: - name: db-data emptyDir: {} db Create new service: ‘db’port: ‘5432’targetPort: ‘5432’type: ‘ClusterIP’ yaml123456789101112apiVersion: v1kind: Servicemetadata: name: db namespace: votespec: ports: - protocol: TCP port: 5432 targetPort: 5432 selector: app: db-deployment result-deployment Create new deployment, name: ‘result-deployment’image: ‘kodekloud/examplevotingapp_result:before’status: ‘Running’ yaml1234567891011121314151617apiVersion: apps/v1kind: Deploymentmetadata: name: result-deployment namespace: vote spec: selector: matchLabels: app: result-deployment template: metadata: labels: app: result-deployment spec: containers: - image: kodekloud/examplevotingapp_result:before name: db-deployment result-service port: ‘5001’targetPort: ‘80’NodePort: ‘31001’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: result-service namespace: votespec: ports: - protocol: TCP port: 5001 targetPort: 80 nodePort: 31001 type: NodePort selector: app: result-deployment","link":"/2020/10/23/Game-Of-Pods-VotingApp/"},{"title":"Game-Of-Pods-Tyro","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods Solutionscreate namespace1kubectl create ns development admin Certificate and key pair for user drogo is created under /root. Add this user to kubeconfig = /root/.kube/config, User = drogo, client-key = /root/drogo.key client-certificate = /root/drogo.crtCreate a new context in the default config file (/root/.kube/config) called ‘developer’ with user = drogo and cluster = kubernetes 12kubectl config set-credentials drogo --client-key=/root/drogo.key --client-certificate=/root/drogo.crtkubectl config set-context developer --cluster=kubernetes --user=drogo developer-role ‘developer-role’, should have all() permissions for services in development namespace‘developer-role’, should have all permissions() for persistentvolumeclaims in development namespace‘developer-role’, should have all(*) permissions for pods in development namespace yaml1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: developer-role namespace: developmentrules: - apiGroups: - &quot;&quot; resources: - svc - pvc - pods verbs: - &quot;*&quot; developer-rolebinding create rolebinding = developer-rolebinding, role= ‘developer-role’, namespace = developmentrolebinding = developer-rolebinding associated with user = ‘drogo’ yaml 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: developer-rolebinding namespace: developmentsubjects: - kind: User name: drogo apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: developer-role apiGroup: rbac.authorization.k8s.io kube-config set context ‘developer’ with user = ‘drogo’ and cluster = ‘kubernetes’ as the current context. 1kubectl config use-context developer jekyll-pvc Storage Request: 1GiAccess modes: ReadWriteManypvc name = jekyll-site, namespace development yaml 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: jekyll-site namespace: developmentspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi jekyll pod: ‘jekyll’ has an initContainer, name: ‘copy-jekyll-site’, image: ‘kodekloud/jekyll’initContainer: ‘copy-jekyll-site’ command: [ “jekyll”, “new”, “/site” ] (command to run: jekyll new /site)pod: ‘jekyll’, initContainer: ‘copy-jekyll-site’, mountPath = /sitepod: ‘jekyll’, initContainer: ‘copy-jekyll-site’, volume name = sitepod: ‘jekyll’, container: ‘jekyll’, volume name = sitepod: ‘jekyll’, container: ‘jekyll’, mountPath = /sitepod: ‘jekyll’, container: ‘jekyll’, image = kodekloud/jekyll-servepod: ‘jekyll’, uses volume called ‘site’ with pvc = ‘jekyll-site’pod: ‘jekyll’ uses label ‘run=jekyll’ yaml12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: namespace: development name: jekyll labels: run: jekyllspec: initContainers: - name: copy-jekyll-site image: kodekloud/jekyll command: [ &quot;jekyll&quot;, &quot;new&quot;, &quot;/site&quot; ] volumeMounts: - mountPath: /site name: site containers: - name: jekyll image: kodekloud/jekyll-serve volumeMounts: - mountPath: /site name: site volumes: - name: site persistentVolumeClaim: claimName: jekyll-site jekyll-node-service Service ‘jekyll’ uses targetPort: ‘4000’ , namespace: ‘development’Service ‘jekyll’ uses Port: ‘8080’ , namespace: ‘development’Service ‘jekyll’ uses NodePort: ‘30097’ , namespace: ‘development’ yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: name: jekyll namespace: development labels: run: jekyllspec: ports: - protocol: TCP port: 8080 targetPort: 4000 nodePort: 30097 selector: run: jekyll type: NodePort","link":"/2020/10/23/Game-Of-Pods-Tyro/"},{"title":"Game-Of-Pods-RedisIslands","text":"KodeKloud-Game Of Podhttps://kodekloud.com/p/game-of-pods SolutionsStatefulset 에 대한 내용을 알고 있어야 한다. redis01~06 PersistentVolume - Name: redis01Access modes: ReadWriteOnceSize: 1GihostPath: /redis01, directory should be created on worker node ssh node01 make /redis01~06 yaml (숫자만 01에서 06까지 바꿔주면 된다)1234567891011apiVersion: v1kind: PersistentVolumemetadata: name: redis01spec: accessModes: - ReadWriteOnce hostPath: path: /redis01 capacity: storage: 1Gi redis-cluster StatefulSet - Name: redis-clusterReplicas: 6Pods status: Running (All 6 replicas)Image: redis:5.0.1-alpinecontainer name: redis, command: [“/conf/update-node.sh”, “redis-server”, “/conf/redis.conf”]Env: name: ‘POD_IP’, valueFrom: ‘fieldRef’, fieldPath: ‘status.podIP’ (apiVersion: v1)Ports - name: ‘client’, containerPort: ‘6379’Ports - name: ‘gossip’, containerPort: ‘16379’Volume Mount - name: ‘conf’, mountPath: ‘/conf’, readOnly:’false’ (ConfigMap Mount)Volume Mount - name: ‘conf’, mountPath: ‘/conf’, defaultMode = ‘0755’ (ConfigMap Mount)Volume Mount - name: ‘data’, mountPath: ‘/data’, readOnly:’false’ (volumeClaim)volumes - name: ‘conf’, Type: ‘ConfigMap’, ConfigMap Name: ‘redis-cluster-configmap’,volumeClaimTemplates - name: ‘data’volumeClaimTemplates - accessModes: ‘ReadWriteOnce’volumeClaimTemplates - Storage Request: ‘1Gi’ yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: apps/v1kind: StatefulSetmetadata: name: redis-clusterspec: selector: matchLabels: app: redis-cluster serviceName: redis-cluster replicas: 6 template: metadata: labels: app: redis-cluster spec: containers: - name: redis image: redis:5.0.1-alpine command: [&quot;/conf/update-node.sh&quot;, &quot;redis-server&quot;, &quot;/conf/redis.conf&quot;] env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP ports: - name: client containerPort: 6379 - name: gossip containerPort: 16379 volumeMounts: - name: conf mountPath: /conf readOnly: false - name: data mountPath: /data readOnly: false volumes: - name: conf configMap: name: redis-cluster-configmap defaultMode: 0755 volumeClaimTemplates: - metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi redis-cluster-config Configure the Cluster. Once the StatefulSet has been deployed with 6 ‘Running’ pods, run the below commands and type ‘yes’ when prompted.Command: kubectl exec -it redis-cluster-0 – redis-cli –cluster create –cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath=’{range.items[*]}{.status.podIP}:6379 ‘) 아래 명령어 실행1kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 ') redis-cluster-service Ports - service name ‘redis-cluster-service’, port name: ‘client’, port: ‘6379’Ports - service name ‘redis-cluster-service’, port name: ‘gossip’, port: ‘16379’Ports - service name ‘redis-cluster-service’, port name: ‘client’, targetPort: ‘6379’Ports - service name ‘redis-cluster-service’, port name: ‘gossip’, targetPort: ‘16379’ yaml1234567891011121314apiVersion: v1kind: Servicemetadata: name: redis-cluster-servicespec: ports: - protocol: TCP name: client port: 6379 targetPort: 6379 - protocol: TCP name: gossip port: 16379 targetPort: 16379","link":"/2020/10/23/Game-Of-Pods-RedisIslands/"}],"tags":[{"name":"K8S","slug":"K8S","link":"/tags/K8S/"},{"name":"PV","slug":"PV","link":"/tags/PV/"},{"name":"PVC","slug":"PVC","link":"/tags/PVC/"},{"name":"쿠버네티스","slug":"쿠버네티스","link":"/tags/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4/"},{"name":"ReplicaSet","slug":"ReplicaSet","link":"/tags/ReplicaSet/"},{"name":"RS","slug":"RS","link":"/tags/RS/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"kodekolud","slug":"kodekolud","link":"/tags/kodekolud/"},{"name":"gameofpod","slug":"gameofpod","link":"/tags/gameofpod/"},{"name":"Bravo","slug":"Bravo","link":"/tags/Bravo/"},{"name":"IronGallery","slug":"IronGallery","link":"/tags/IronGallery/"},{"name":"Pento","slug":"Pento","link":"/tags/Pento/"},{"name":"cordon","slug":"cordon","link":"/tags/cordon/"},{"name":"uncordon","slug":"uncordon","link":"/tags/uncordon/"},{"name":"VotingApp","slug":"VotingApp","link":"/tags/VotingApp/"},{"name":"Tyro","slug":"Tyro","link":"/tags/Tyro/"},{"name":"RedisIslands","slug":"RedisIslands","link":"/tags/RedisIslands/"}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"}]}